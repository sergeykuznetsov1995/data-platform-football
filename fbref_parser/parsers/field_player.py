"""
Field player parser for FBref statistics

This module implements the FieldPlayerParser class for parsing field player
statistics from FBref.com, including support for individual players and full squads.
"""

import pandas as pd
import time
import os
from io import StringIO
from typing import Dict, List, Optional

from .base_parser import BaseParser
from ..core.scraper import FBrefScraper, extract_all_tables
from ..core.table_detector import (
    identify_field_player_tables,
    find_tables_by_unique_markers,
    resolve_table_conflict,
    analyze_all_tables
)
from ..core.column_processor import apply_field_player_renames
from ..utils.url_helpers import extract_player_name_from_url
from ..utils.file_helpers import get_output_path, normalize_name
from ..utils.squad_helpers import extract_field_player_links
from .. import constants


class FieldPlayerParser(BaseParser):
    """
    Parser for field player statistics from FBref

    Handles parsing of individual field players or entire squads,
    including all statistical categories (standard, shooting, passing, etc.)
    """

    def __init__(self):
        """Initialize field player parser"""
        super().__init__()
        self.scraper = FBrefScraper()

    def identify_tables(self, all_tables: List[pd.DataFrame]) -> Dict:
        """
        Identify field player tables using content-based detection

        Args:
            all_tables: List of all tables from page

        Returns:
            Dictionary mapping table type to (index, DataFrame)
        """
        return identify_field_player_tables(all_tables)

    def apply_specific_renames(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Apply field player-specific column renames

        Args:
            df: DataFrame to process

        Returns:
            DataFrame with field player renames applied
        """
        return apply_field_player_renames(df)

    def get_no_prefix_tables(self) -> List[str]:
        """
        Get list of tables that should not receive column prefixes

        Returns:
            List containing 'standard'
        """
        return ['standard']

    def parse_player(self, player_url: str, player_name: str = None,
                    output_path: str = None, simple_filename: bool = False) -> Optional[pd.DataFrame]:
        """
        Parse individual field player statistics

        Args:
            player_url: URL of the player's all_comps page
            player_name: Player name (optional, extracted from URL if not provided)
            output_path: Custom output path (optional)
            simple_filename: If True, uses simple filename without suffix

        Returns:
            DataFrame with parsed statistics, or None if parsing failed
        """
        if not player_name:
            player_name = extract_player_name_from_url(player_url)

        print(f"üöÄ –ó–∞–ø—É—Å–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è {player_name}...")

        try:
            print("üì• –ó–∞–≥—Ä—É–∂–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É...")
            response = self.scraper.fetch_page(player_url)

            # Extract all tables from page
            print("üîç –ò—â—É –≤—Å–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–∞–±–ª–∏—Ü—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ...")
            all_page_tables = extract_all_tables(response.content.decode('utf-8'))
            print(f"–ù–∞–π–¥–µ–Ω–æ {len(all_page_tables)} —Ç–∞–±–ª–∏—Ü –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")

            # Identify key tables by content
            key_tables = self.identify_tables(all_page_tables)

            # Fallback mechanism for finding missing tables
            expected_tables = ['standard', 'shooting', 'passing', 'pass_types', 'gca', 'defense', 'possession', 'playing_time', 'misc']
            missing_tables = [t for t in expected_tables if t not in key_tables]

            if missing_tables:  # Always try to find missing tables
                print(f"\n‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ —Ç–æ–ª—å–∫–æ {len(key_tables)} —Ç–∞–±–ª–∏—Ü –∏–∑ {len(expected_tables)} –æ–∂–∏–¥–∞–µ–º—ã—Ö")
                print(f"–ù–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Ç–∞–±–ª–∏—Ü—ã: {', '.join(missing_tables)}")
                print("–ü—ã—Ç–∞—é—Å—å –Ω–∞–π—Ç–∏ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Ç–∞–±–ª–∏—Ü—ã –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –º–∞—Ä–∫–µ—Ä–∞–º...")

                # Find by unique markers
                found_by_markers = find_tables_by_unique_markers(all_page_tables, missing_tables)

                # Add found tables with conflict resolution
                rejected_tables = []
                for table_name, (pos, table) in found_by_markers.items():
                    # Check for position conflicts
                    conflict_name = None
                    for existing_name, (existing_pos, existing_table) in key_tables.items():
                        if existing_pos == pos:
                            conflict_name = existing_name
                            break

                    if conflict_name:
                        # Resolve conflict - choose best table
                        chosen_name, chosen_table, rejected_name = resolve_table_conflict(
                            conflict_name, key_tables[conflict_name][1],
                            table_name, table,
                            pos
                        )

                        # Update key_tables with chosen table
                        if chosen_name == table_name:
                            # New table is better - replace old one
                            del key_tables[conflict_name]
                            key_tables[table_name] = (pos, chosen_table)
                            rejected_tables.append(rejected_name)
                        else:
                            # Old one is better - remember rejected new one
                            rejected_tables.append(rejected_name)
                    else:
                        # No conflict - just add
                        key_tables[table_name] = (pos, table)

                # Search for rejected tables at other positions
                if rejected_tables:
                    print(f"\nüîÑ –ò—â—É –æ—Ç–∫–ª–æ–Ω—ë–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã –Ω–∞ –¥—Ä—É–≥–∏—Ö –ø–æ–∑–∏—Ü–∏—è—Ö: {rejected_tables}")
                    for rejected_table in rejected_tables:
                        found_alternatives = find_tables_by_unique_markers(all_page_tables, [rejected_table])

                        for alt_name, (alt_pos, alt_table) in found_alternatives.items():
                            # Check that position is free
                            pos_occupied = any(existing_pos == alt_pos for existing_name, (existing_pos, existing_table) in key_tables.items())
                            if not pos_occupied:
                                key_tables[alt_name] = (alt_pos, alt_table)
                                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è –¥–ª—è {alt_name} #{alt_pos}")
                                break

            # Check final list of missing tables
            final_missing = [t for t in expected_tables if t not in key_tables]

            if final_missing:
                print(f"\n‚ö†Ô∏è –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü: {', '.join(final_missing)}")
                print("–ó–∞–ø—É—Å–∫–∞—é –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü...")
                analyze_all_tables(all_page_tables)

            # Check if we have at least the 'standard' table (mandatory)
            if 'standard' not in key_tables:
                print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ STANDARD")
                if not key_tables:
                    print("   –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π –∫–ª—é—á–µ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã")
                return None

            # Warn about missing tables but continue parsing
            if final_missing:
                print(f"‚ö†Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∞—é –ø–∞—Ä—Å–∏–Ω–≥ —Å {len(key_tables)} —Ç–∞–±–ª–∏—Ü–∞–º–∏ (–æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç: {', '.join(final_missing)})")

            print(f"\nüîó –ù–∞–π–¥–µ–Ω–æ {len(key_tables)} –∫–ª—é—á–µ–≤—ã—Ö —Ç–∞–±–ª–∏—Ü –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è")

            # Process each table
            processed_tables = {}

            for table_name, (table_idx, table) in key_tables.items():
                print(f"\nüìä –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ç–∞–±–ª–∏—Ü—É {table_name}...")
                processed_table = self.process_table_columns(table.copy(), table_name)
                processed_tables[table_name] = processed_table
                print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(processed_table)} —Å—Ç—Ä–æ–∫ –∏–∑ {table_name}")

            # Merge all tables
            merged_df = self.merge_tables(processed_tables)

            # Final cleanup
            final_df = self.final_cleanup(merged_df)

            # Apply field player-specific renames
            final_df = self.apply_specific_renames(final_df)

            # Determine output path
            if not output_path:
                output_path = get_output_path(
                    player_name,
                    output_dir=constants.DEFAULT_OUTPUT_DIR_FIELD_PLAYERS if simple_filename else None,
                    simple_filename=simple_filename
                )

            # Save to CSV
            self.save_to_csv(final_df, output_path)

            # Show sample data
            print(f"\nüìã –û–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö (–ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫):")
            season_col = squad_col = comp_col = None

            for col in final_df.columns:
                col_lower = str(col).lower()
                if 'season' in col_lower and season_col is None:
                    season_col = col
                elif 'squad' in col_lower and squad_col is None:
                    squad_col = col
                elif ('comp' in col_lower or 'tournament' in col_lower) and comp_col is None:
                    comp_col = col

            if season_col and squad_col and comp_col:
                sample_data = final_df[[season_col, squad_col, comp_col]].head(10)
                print(sample_data.to_string(index=False))
            else:
                print(final_df.iloc[:10, :3].to_string(index=False))
                print(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –ø–µ—Ä–≤—ã–µ 3 –∫–æ–ª–æ–Ω–∫–∏: {list(final_df.columns[:3])}")

            return final_df

        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            return None

    def parse_squad(self, squad_url: str, limit: int = None, delay: int = 4) -> int:
        """
        Parse all field players from squad

        Args:
            squad_url: URL of the squad page
            limit: Maximum number of players to parse (optional, for testing)
            delay: Delay between requests in seconds (default: 4)

        Returns:
            Number of successfully parsed players
        """
        print(f"üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ –∫–æ–º–∞–Ω–¥—ã...")
        print(f"üìç URL –∫–æ–º–∞–Ω–¥—ã: {squad_url}")

        # Extract links to all field players
        player_links = extract_field_player_links(squad_url)

        if not player_links:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø–æ–ª–µ–≤–æ–≥–æ –∏–≥—Ä–æ–∫–∞")
            return 0

        # Apply limit if specified
        if limit and limit > 0:
            player_links = player_links[:limit]
            print(f"‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: –±—É–¥–µ—Ç —Å–ø–∞—Ä—à–µ–Ω–æ —Ç–æ–ª—å–∫–æ {len(player_links)} –∏–≥—Ä–æ–∫–æ–≤")

        # Get list of existing files to skip already parsed players
        existing_files = set()
        if os.path.exists(constants.DEFAULT_OUTPUT_DIR_FIELD_PLAYERS):
            existing_files = set(os.listdir(constants.DEFAULT_OUTPUT_DIR_FIELD_PLAYERS))

        successful_parses = 0
        failed_parses = 0
        skipped_players = 0

        print(f"\nüîÑ –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ {len(player_links)} –ø–æ–ª–µ–≤—ã—Ö –∏–≥—Ä–æ–∫–æ–≤...")
        if existing_files:
            print(f"üìÇ –ù–∞–π–¥–µ–Ω–æ {len(existing_files)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ - –æ–Ω–∏ –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã")

        for i, (player_name, player_url) in enumerate(player_links, 1):
            # Check if player file already exists
            player_filename = f"{normalize_name(player_name)}.csv"
            if player_filename in existing_files:
                print(f"\n‚è≠Ô∏è  –ò–≥—Ä–æ–∫ {i}/{len(player_links)}: {player_name} - —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞—é")
                skipped_players += 1
                continue

            print(f"\nüìä –ü–∞—Ä—Å–∏–Ω–≥ –∏–≥—Ä–æ–∫–∞ {i}/{len(player_links)}: {player_name}")

            try:
                result = self.parse_player(
                    player_url=player_url,
                    player_name=player_name,
                    output_path=None,
                    simple_filename=True
                )

                if result is not None:
                    successful_parses += 1
                    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—à–µ–Ω: {player_name}")
                else:
                    failed_parses += 1
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {player_name}")

            except Exception as e:
                failed_parses += 1
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {player_name}: {e}")

            # Delay between requests (except for last player)
            if i < len(player_links):
                print(f"‚è≥ –ó–∞–¥–µ—Ä–∂–∫–∞ {delay} —Å–µ–∫—É–Ω–¥...")
                time.sleep(delay)

        # Final statistics
        print(f"\nüéâ –ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–º–∞–Ω–¥—ã –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—à–µ–Ω–æ: {successful_parses} –∏–≥—Ä–æ–∫–æ–≤")
        print(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ (—É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç): {skipped_players} –∏–≥—Ä–æ–∫–æ–≤")
        print(f"‚ùå –û—à–∏–±–æ–∫ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ: {failed_parses} –∏–≥—Ä–æ–∫–æ–≤")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {constants.DEFAULT_OUTPUT_DIR_FIELD_PLAYERS}")

        return successful_parses
