"""
Goalkeeper parser for FBref statistics

This module implements the GoalkeeperParser class for parsing goalkeeper
statistics from FBref.com, including goalkeeper-specific tables and all field player stats.
"""

import pandas as pd
import time
from io import StringIO
from typing import Dict, List, Optional

from .base_parser import BaseParser
from ..core.scraper import FBrefScraper
from ..core.table_detector import identify_goalkeeper_tables
from ..core.column_processor import apply_goalkeeper_renames
from ..utils.file_helpers import get_output_path, normalize_name
from ..utils.squad_helpers import extract_goalkeeper_links
from ..constants import DEFAULT_OUTPUT_DIR_GOALKEEPERS


class GoalkeeperParser(BaseParser):
    """
    Parser for goalkeeper statistics from FBref

    Handles parsing of individual goalkeepers or entire squads,
    including goalkeeper-specific stats (saves, clean sheets, PSxG)
    and all field player statistics.
    """

    def __init__(self):
        """Initialize goalkeeper parser"""
        super().__init__()
        self.scraper = FBrefScraper()

    def identify_tables(self, all_tables: List[pd.DataFrame]) -> Dict:
        """
        Identify goalkeeper tables including GK-specific categories

        Args:
            all_tables: List of all tables from page

        Returns:
            Dictionary mapping table type to list of table info
        """
        return identify_goalkeeper_tables(all_tables)

    def apply_specific_renames(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Apply goalkeeper-specific column renames

        Args:
            df: DataFrame to process

        Returns:
            DataFrame with goalkeeper renames applied
        """
        return apply_goalkeeper_renames(df)

    def get_no_prefix_tables(self) -> List[str]:
        """
        Get list of tables that should not receive column prefixes

        For goalkeepers, standard, goalkeeping, and advanced_goalkeeping
        tables don't get prefixes.

        Returns:
            List of table types without prefixes
        """
        return ['standard', 'goalkeeping', 'advanced_goalkeeping']

    def parse_goalkeeper(self, player_name: str, player_url: str,
                        output_path: str = None) -> Optional[pd.DataFrame]:
        """
        Parse individual goalkeeper statistics

        Args:
            player_name: Goalkeeper name
            player_url: URL of the goalkeeper's all_comps page
            output_path: Custom output path (optional)

        Returns:
            DataFrame with parsed statistics, or None if parsing failed
        """
        print(f"\nü•Ö –ü–∞—Ä—Å–∏–Ω–≥ {player_name}...")
        print(f"üîó URL: {player_url}")

        try:
            response = self.scraper.fetch_page(player_url)
            time.sleep(1)  # Respectful delay for server

            # Read all tables from page
            all_tables = pd.read_html(StringIO(response.text), encoding='utf-8')
            print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(all_tables)} —Ç–∞–±–ª–∏—Ü –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")

            # Identify goalkeeper-specific tables
            identified_tables = self.identify_tables(all_tables)

            # Output info about found tables
            for table_type, tables in identified_tables.items():
                if tables:
                    print(f"   {table_type}: {len(tables)} —Ç–∞–±–ª–∏—Ü")

            # Merge all found data
            all_data = []

            for table_type, tables in identified_tables.items():
                if not tables:
                    continue

                # Choose best table for each type (with most matches)
                best_table = max(tables, key=lambda x: x['matches'])
                table_data = best_table['table'].copy()

                print(f"   –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {table_type} (—Ç–∞–±–ª–∏—Ü–∞ {best_table['index']})...")

                # Process table columns
                processed_table = self.process_table_columns(table_data, table_type)

                if not processed_table.empty:
                    all_data.append(processed_table)

            if not all_data:
                print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Ä–∞—Ç–∞—Ä—è")
                return None

            # Merge all tables
            print("üîó –û–±—ä–µ–¥–∏–Ω—è—é —Ç–∞–±–ª–∏—Ü—ã...")

            # Key columns for merging
            key_columns = ['Season', 'Age', 'Squad', 'Country', 'Comp']

            # Start with first table
            merged_data = all_data[0]

            # Merge remaining tables
            for i in range(1, len(all_data)):
                try:
                    # Find common key columns
                    key_columns_present = [col for col in key_columns if col in merged_data.columns and col in all_data[i].columns]

                    if key_columns_present:
                        # Merge by key columns with automatic suffixes
                        merged_data = pd.merge(
                            merged_data,
                            all_data[i],
                            on=key_columns_present,
                            how='outer',
                            suffixes=('', '_dup')
                        )
                        print(f"   –û–±—ä–µ–¥–∏–Ω–∏–ª —Ç–∞–±–ª–∏—Ü—É {i+1} –ø–æ –∫–ª—é—á–∞–º: {key_columns_present}")
                    else:
                        # If no common keys, concatenate by indices
                        merged_data = pd.concat([merged_data, all_data[i]], axis=1)
                        print(f"   –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞–ª —Ç–∞–±–ª–∏—Ü—É {i+1} –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º")

                except Exception as e:
                    print(f"   ‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É {i+1}: {e}")
                    try:
                        merged_data = pd.concat([merged_data, all_data[i]], axis=1)
                        print(f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—é –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã {i+1}")
                    except Exception as e2:
                        print(f"   ‚ùå –ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É {i+1}: {e2}")
                        continue

            # Remove duplicate columns after merging
            print("   –£–¥–∞–ª—è—é –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —Å—Ç–æ–ª–±—Ü—ã...")

            # Remove columns with _dup suffix
            dup_columns = [col for col in merged_data.columns if str(col).endswith('_dup')]
            if dup_columns:
                merged_data = merged_data.drop(columns=dup_columns, errors='ignore')
                print(f"   –£–¥–∞–ª–µ–Ω–æ {len(dup_columns)} –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —Å—Ç–æ–ª–±—Ü–æ–≤ —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º _dup")

            # Remove completely identical columns
            merged_data = merged_data.loc[:, ~merged_data.columns.duplicated()]

            # Final data processing
            final_data = self.apply_specific_renames(merged_data)

            print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°—Ç—Ä–æ–∫: {len(final_data)}, –°—Ç–æ–ª–±—Ü–æ–≤: {len(final_data.columns)}")

            # Save if output path provided
            if output_path:
                final_data.to_csv(output_path, index=False, encoding='utf-8')
                print(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")

            return final_data

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {player_name}: {e}")
            return None

    def parse_squad_goalkeepers(self, squad_url: str) -> int:
        """
        Parse all goalkeepers from squad

        Args:
            squad_url: URL of the squad page

        Returns:
            Number of successfully parsed goalkeepers
        """
        print("üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –≤—Ä–∞—Ç–∞—Ä–µ–π...")

        # Get goalkeeper links
        goalkeeper_links = extract_goalkeeper_links(squad_url)

        if not goalkeeper_links:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω—ã –≤—Ä–∞—Ç–∞—Ä–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
            return 0

        successful_parses = 0

        for i, (player_name, player_url) in enumerate(goalkeeper_links):
            print(f"\n{'='*60}")
            print(f"ü•Ö –í—Ä–∞—Ç–∞—Ä—å {i+1}/{len(goalkeeper_links)}: {player_name}")

            # Parse goalkeeper stats
            player_data = self.parse_goalkeeper(player_name, player_url)

            if player_data is not None and not player_data.empty:
                # Save data to CSV
                normalized_name = normalize_name(player_name)
                output_path = f"{DEFAULT_OUTPUT_DIR_GOALKEEPERS}/{normalized_name}_goalkeeper_stats.csv"

                try:
                    player_data.to_csv(output_path, index=False, encoding='utf-8')
                    print(f"üíæ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")
                    successful_parses += 1

                    # Output brief statistics
                    print(f"üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {len(player_data)} —Å–µ–∑–æ–Ω–æ–≤, {len(player_data.columns)} –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π")

                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–ª—è {player_name}: {e}")

            # Delay between requests
            if i < len(goalkeeper_links) - 1:
                time.sleep(2)

        print(f"\nüéØ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {successful_parses}/{len(goalkeeper_links)} –≤—Ä–∞—Ç–∞—Ä–µ–π")
        print(f"üìÅ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {DEFAULT_OUTPUT_DIR_GOALKEEPERS}")

        return successful_parses
